{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c974107",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) using Ollama and FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fc48a2",
   "metadata": {},
   "source": [
    "This project is a Python-based Retrieval-Augmented Generation (RAG) system that uses a local Ollama model to answer questions based on a provided PDF document. The system works by first extracting text from the PDF and splitting it into manageable chunks. These text chunks are then converted into numerical embeddings using a Sentence Transformer model. The embeddings are stored in a FAISS index, which enables fast and efficient similarity searches.\n",
    "\n",
    "When a user submits a query, the system retrieves the most relevant text chunks from the FAISS index. This retrieved context is then provided to the Ollama large language model along with the original question. The LLM uses this specific, relevant information to formulate a concise and accurate answer, effectively grounding its response in the document's content. This process allows the system to provide knowledgeable answers without hallucinating, as it is restricted to the information found in the PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e9d32",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19050c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import InferenceClient\n",
    "import openai\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e9679",
   "metadata": {},
   "source": [
    "### Defines the file path for the PDF document that will be used as the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2761c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pdf (math chapter)\n",
    "file = \"data/data.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a78a8",
   "metadata": {},
   "source": [
    "### Uses the fitz library to open and read the PDF file. It iterates through each page to extract all text content and prints the total number of characters extracted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26bdd6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 69078 characters from PDF.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = file\n",
    "# Load the PDF file\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "text = \"\"\n",
    "for page in doc:\n",
    "    text += page.get_text()\n",
    "\n",
    "print(f\"Extracted {len(text)} characters from PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f620a8",
   "metadata": {},
   "source": [
    "### Splits the extracted text into smaller, manageable chunks of 500 characters using textwrap.wrap. This is an important step to prepare the data for embedding and retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84134179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 139 chunks.\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 500 \n",
    "chunks = wrap(text, chunk_size)\n",
    "print(f\"Document split into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb72f0c3",
   "metadata": {},
   "source": [
    "### Initializes a pre-trained SentenceTransformer model (all-MiniLM-L6-v2) to convert the text chunks into numerical vectors (embeddings). It then encodes each chunk and stores the resulting embeddings in a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7e641f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (139, 384)\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings = [embedding_model.encode(chunk) for chunk in chunks]\n",
    "embeddings = np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "print(f\"Generated embeddings with shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccce50b1",
   "metadata": {},
   "source": [
    "### Imports the faiss library, a highly efficient library for similarity search. It then creates a Faiss index (IndexFlatL2) and adds all the generated embeddings to it, making them searchable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3b23b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 139 chunks.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "embedding_dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"FAISS index created with\", index.ntotal, \"chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da90764",
   "metadata": {},
   "source": [
    "### Defines the search function. This function takes a user query, encodes it into an embedding, and uses the Faiss index to find the top k most similar text chunks based on L2 distance. It returns the chunks and their corresponding distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11637471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, top_k=3):\n",
    "    query_emb = embedding_model.encode(query)\n",
    "    query_emb = np.array([query_emb], dtype=\"float32\")\n",
    "    distances, indices = index.search(query_emb, top_k)\n",
    "    return [(chunks[i], distances[0][pos]) for pos, i in enumerate(indices[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d96dca",
   "metadata": {},
   "source": [
    "### Imports the Client from the ollama library, which is used to interact with a local Ollama server running a large language model. It defines the answer_question function, which orchestrates the RAG process. This function retrieves relevant context using the search function, constructs a prompt for the LLM using that context, and sends the prompt to the llama3 model to generate a final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "def answer_question(query):\n",
    "    # Retrieve context\n",
    "    relevant_chunks = search(query, top_k=3)\n",
    "    context = \"\\n\".join([chunk for chunk, _ in relevant_chunks])\n",
    "\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an AI assistant that should only rely on the supplied context when answering.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Rely on the context below to respond to the question.\n",
    "Guidelines:\n",
    "1. If the answer is not in the context, reply with \"I don't know\".\n",
    "2. Keep your response shortâ€”no more than five sentences.\n",
    "3. Use strictly the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "    response = client.chat(\n",
    "        model=\"llama3\",  # replace with your local model\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    # Access the content properly\n",
    "    return response.message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c4466",
   "metadata": {},
   "source": [
    "### This is the example usage of the answer_question function. It demonstrates how to call the function with a specific query, \"What is a Pre-trained neural language model?\". The code then prints the final answer generated by the RAG system, which includes the retrieval of relevant context and the subsequent generation of a response by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eec4ca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, a pre-trained neural language model refers to a single, left-to-right or encoder-decoder model that can achieve strong performance across both discriminative and generative tasks. This type of model is trained on a large corpus of text data before being fine-tuned for specific tasks.\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "print(answer_question(\"What is a Pre-trained neural language model?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
